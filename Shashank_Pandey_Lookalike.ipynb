{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 15:47:44.557892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738059464.577375   96407 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738059464.582167   96407 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-28 15:47:44.599505: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookalikeModel:\n",
    "    def __init__(self):\n",
    "        self.transactions_df = pd.read_csv('Transactions.csv')\n",
    "        self.products_df = pd.read_csv('Products.csv')\n",
    "        self.customers_df = pd.read_csv('Customers.csv')\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Split transactions into train and test sets (80-20 split)\n",
    "        cutoff_date = pd.Timestamp('2024-10-01')  # Using Oct 1, 2024 as cutoff\n",
    "        self.train_transactions = self.transactions_df[\n",
    "            pd.to_datetime(self.transactions_df['TransactionDate']) < cutoff_date\n",
    "        ]\n",
    "        self.test_transactions = self.transactions_df[\n",
    "            pd.to_datetime(self.transactions_df['TransactionDate']) >= cutoff_date\n",
    "        ]\n",
    "        \n",
    "    def create_customer_profiles(self, transactions_df=None):\n",
    "        \"\"\"\n",
    "        Enhanced customer profile creation with better features and weights\n",
    "        \"\"\"\n",
    "        if transactions_df is None:\n",
    "            transactions_df = self.train_transactions\n",
    "            \n",
    "        # Merge transactions with products\n",
    "        trans_prod = transactions_df.merge(self.products_df, on='ProductID')\n",
    "        \n",
    "        # Convert datetime columns\n",
    "        trans_prod['TransactionDate'] = pd.to_datetime(trans_prod['TransactionDate'])\n",
    "        self.customers_df['SignupDate'] = pd.to_datetime(self.customers_df['SignupDate'])\n",
    "        \n",
    "        # Calculate time-based features\n",
    "        current_date = pd.Timestamp('2024-12-31')\n",
    "        trans_prod['days_since_signup'] = (trans_prod['TransactionDate'] - \n",
    "                                         pd.to_datetime(self.customers_df.set_index('CustomerID')\n",
    "                                         .loc[trans_prod['CustomerID'], 'SignupDate'].values)).dt.days\n",
    "        \n",
    "        # Calculate price segments and purchase behavior\n",
    "        trans_prod['avg_price'] = trans_prod['TotalValue'] / trans_prod['Quantity']\n",
    "        trans_prod['price_segment'] = pd.qcut(trans_prod['avg_price'], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "        \n",
    "        # Enhanced RFM with time weights\n",
    "        customer_rfm = trans_prod.groupby('CustomerID').agg({\n",
    "            'TransactionDate': lambda x: np.exp(-0.01 * (current_date - x.max()).days),  # Exponential decay for recency\n",
    "            'TransactionID': 'count',\n",
    "            'TotalValue': 'sum',\n",
    "            'days_since_signup': 'max'  # Customer age\n",
    "        }).rename(columns={\n",
    "            'TransactionDate': 'recency_score',\n",
    "            'TransactionID': 'frequency',\n",
    "            'TotalValue': 'monetary',\n",
    "            'days_since_signup': 'customer_age'\n",
    "        })\n",
    "        \n",
    "        # Normalize monetary value by customer age\n",
    "        customer_rfm['monetary_per_day'] = customer_rfm['monetary'] / customer_rfm['customer_age'].clip(lower=1)\n",
    "        \n",
    "        # Purchase patterns with price segments\n",
    "        purchase_patterns = trans_prod.groupby('CustomerID').agg({\n",
    "            'Quantity': ['sum', 'mean', 'std'],\n",
    "            'TotalValue': ['mean', 'std'],\n",
    "            'avg_price': ['mean', 'std', 'min', 'max']\n",
    "        })\n",
    "        purchase_patterns.columns = [f\"{col[0]}_{col[1]}\" for col in purchase_patterns.columns]\n",
    "        \n",
    "        # Price segment preferences\n",
    "        price_segments = pd.get_dummies(trans_prod[['CustomerID', 'price_segment']]\n",
    "                                      .set_index('CustomerID')).groupby('CustomerID').mean()\n",
    "        \n",
    "        # Category preferences with value weights\n",
    "        category_totals = trans_prod.groupby('Category')['TotalValue'].sum()\n",
    "        category_weights = 1 / np.log1p(category_totals)  # Inverse log weighting\n",
    "        \n",
    "        # Create category features\n",
    "        category_features = []\n",
    "        for category in trans_prod['Category'].unique():\n",
    "            # Total value per category\n",
    "            cat_value = trans_prod[trans_prod['Category'] == category].groupby('CustomerID')['TotalValue'].sum()\n",
    "            cat_value = cat_value * category_weights[category]\n",
    "            cat_value.name = f'weighted_value_{category}'\n",
    "            category_features.append(cat_value)\n",
    "            \n",
    "            # Quantity per category\n",
    "            cat_qty = trans_prod[trans_prod['Category'] == category].groupby('CustomerID')['Quantity'].sum()\n",
    "            cat_qty.name = f'quantity_{category}'\n",
    "            category_features.append(cat_qty)\n",
    "        \n",
    "        category_df = pd.concat(category_features, axis=1).fillna(0)\n",
    "        \n",
    "        # Customer demographics with region similarity\n",
    "        region_dummies = pd.get_dummies(\n",
    "            self.customers_df[['CustomerID', 'Region']], \n",
    "            columns=['Region'],\n",
    "            prefix='region'\n",
    "        ).set_index('CustomerID')\n",
    "        \n",
    "        # Combine all features\n",
    "        customer_profiles = (\n",
    "            customer_rfm\n",
    "            .join(purchase_patterns)\n",
    "            .join(price_segments)\n",
    "            .join(category_df)\n",
    "            .join(region_dummies)\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(customer_profiles)\n",
    "        \n",
    "        return pd.DataFrame(\n",
    "            scaled_features,\n",
    "            index=customer_profiles.index,\n",
    "            columns=customer_profiles.columns\n",
    "        )\n",
    "    \n",
    "    def create_collaborative_filtering_matrix(self):\n",
    "        print(\"Creating user-item matrix...\")\n",
    "        user_item_matrix = pd.pivot_table(\n",
    "            self.transactions_df,\n",
    "            values='Quantity',\n",
    "            index='CustomerID',\n",
    "            columns='ProductID',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        print(f\"User-item matrix shape: {user_item_matrix.shape}\")\n",
    "        \n",
    "        # Convert to float type\n",
    "        user_item_matrix = user_item_matrix.astype(np.float32)\n",
    "        \n",
    "        # Calculate number of components\n",
    "        k = min(50, min(user_item_matrix.shape) - 1)\n",
    "        print(f\"Computing SVD with {k} components...\")\n",
    "        \n",
    "        # Apply SVD\n",
    "        U, sigma, Vt = svds(user_item_matrix.values, k=k)\n",
    "        sigma = np.diag(sigma)\n",
    "        \n",
    "        # Transform user profiles to latent space\n",
    "        user_latent_features = np.dot(U, np.sqrt(sigma))\n",
    "        return pd.DataFrame(user_latent_features, index=user_item_matrix.index)\n",
    "    \n",
    "    def get_lookalikes(self, customer_profiles, target_customers, n_recommendations=3):\n",
    "        \"\"\"\n",
    "        Enhanced lookalike model with multiple similarity measures\n",
    "        \"\"\"\n",
    "        # Split features into different aspects\n",
    "        cols = customer_profiles.columns\n",
    "        rfm_cols = [col for col in cols if any(x in col.lower() for x in ['recency', 'frequency', 'monetary'])]\n",
    "        price_cols = [col for col in cols if any(x in col.lower() for x in ['price', 'value'])]\n",
    "        category_cols = [col for col in cols if 'TotalValue_' in col]\n",
    "        region_cols = [col for col in cols if 'region_' in col]\n",
    "        \n",
    "        # Calculate similarity matrices for different aspects\n",
    "        rfm_sim = cosine_similarity(customer_profiles[rfm_cols])\n",
    "        price_sim = cosine_similarity(customer_profiles[price_cols])\n",
    "        category_sim = cosine_similarity(customer_profiles[category_cols])\n",
    "        region_sim = cosine_similarity(customer_profiles[region_cols])\n",
    "        \n",
    "        # Combine similarities with weights\n",
    "        similarity_matrix = (\n",
    "            0.3 * rfm_sim +\n",
    "            0.3 * price_sim +\n",
    "            0.3 * category_sim +\n",
    "            0.1 * region_sim\n",
    "        )\n",
    "        \n",
    "        similarity_df = pd.DataFrame(\n",
    "            similarity_matrix,\n",
    "            index=customer_profiles.index,\n",
    "            columns=customer_profiles.index\n",
    "        )\n",
    "        \n",
    "        # Get recommendations\n",
    "        results = {}\n",
    "        for customer in target_customers:\n",
    "            if customer not in similarity_df.index:\n",
    "                continue\n",
    "            \n",
    "            # Get similarity scores excluding self\n",
    "            scores = similarity_df[customer].sort_values(ascending=False)\n",
    "            scores = scores[scores.index != customer]\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_similar = scores[:n_recommendations]\n",
    "            results[customer] = [(cust, round(score, 3)) for cust, score in top_similar.items()]\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def save_recommendations(self, recommendations):\n",
    "        output_rows = []\n",
    "        for customer, recs in recommendations.items():\n",
    "            recs_str = ';'.join([f\"{cust}:{score}\" for cust, score in recs])\n",
    "            output_rows.append({\n",
    "                'CustomerID': customer,\n",
    "                'Lookalikes': recs_str\n",
    "            })\n",
    "        \n",
    "        output_df = pd.DataFrame(output_rows)\n",
    "        output_df.to_csv('Lookalike.csv', index=False)\n",
    "\n",
    "    def evaluate_recommendations(self, recommendations, test_transactions):\n",
    "        \"\"\"\n",
    "        Evaluate recommendation accuracy using test data\n",
    "        Returns precision@k, recall@k, and F1@k for different k values\n",
    "        \"\"\"\n",
    "        # Get actual customer interactions from test data\n",
    "        actual_interactions = {}\n",
    "        for customer in recommendations:\n",
    "            customer_transactions = test_transactions[\n",
    "                test_transactions['CustomerID'] == customer\n",
    "            ]\n",
    "            # Get unique customers who bought the same products\n",
    "            similar_customers = set()\n",
    "            for product in customer_transactions['ProductID'].unique():\n",
    "                product_customers = set(\n",
    "                    test_transactions[\n",
    "                        test_transactions['ProductID'] == product\n",
    "                    ]['CustomerID']\n",
    "                )\n",
    "                similar_customers.update(product_customers)\n",
    "            similar_customers.discard(customer)  # Remove self\n",
    "            actual_interactions[customer] = similar_customers\n",
    "            \n",
    "        # Calculate metrics for different k values\n",
    "        k_values = [3, 5, 10]\n",
    "        metrics = {}\n",
    "        \n",
    "        for k in k_values:\n",
    "            total_precision = 0\n",
    "            total_recall = 0\n",
    "            total_f1 = 0\n",
    "            valid_customers = 0\n",
    "            \n",
    "            for customer, actual in actual_interactions.items():\n",
    "                if customer not in recommendations or not actual:\n",
    "                    continue\n",
    "                    \n",
    "                # Get top k recommended customers\n",
    "                recommended = set([rec[0] for rec in recommendations[customer][:k]])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                if len(recommended) > 0:\n",
    "                    precision = len(recommended.intersection(actual)) / len(recommended)\n",
    "                    recall = len(recommended.intersection(actual)) / len(actual)\n",
    "                    \n",
    "                    # Calculate F1 score\n",
    "                    if precision + recall > 0:\n",
    "                        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "                    else:\n",
    "                        f1 = 0.0\n",
    "                    \n",
    "                    total_precision += precision\n",
    "                    total_recall += recall\n",
    "                    total_f1 += f1\n",
    "                    valid_customers += 1\n",
    "            \n",
    "            # Calculate averages\n",
    "            if valid_customers > 0:\n",
    "                metrics[f'precision@{k}'] = total_precision / valid_customers\n",
    "                metrics[f'recall@{k}'] = total_recall / valid_customers\n",
    "                metrics[f'f1@{k}'] = total_f1 / valid_customers\n",
    "            else:\n",
    "                metrics[f'precision@{k}'] = 0.0\n",
    "                metrics[f'recall@{k}'] = 0.0\n",
    "                metrics[f'f1@{k}'] = 0.0\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "    def print_evaluation_results(self, metrics, method_name=\"Hybrid\"):\n",
    "        \"\"\"\n",
    "        Enhanced print function with detailed algorithm names\n",
    "        \"\"\"\n",
    "        if \"Traditional\" in method_name:\n",
    "            algo_details = (\n",
    "                \"Traditional Method (Weighted Combination):\\n\"\n",
    "                \"- RFM Similarity (30%)\\n\"\n",
    "                \"- Price-based Similarity (30%)\\n\"\n",
    "                \"- Category Preference Similarity (30%)\\n\"\n",
    "                \"- Regional Similarity (10%)\"\n",
    "            )\n",
    "        else:\n",
    "            algo_details = (\n",
    "                \"Hybrid ML Method:\\n\"\n",
    "                \"- Traditional Features (40%)\\n\"\n",
    "                \"- Matrix Factorization/SVD (30%)\\n\"\n",
    "                \"- Neural Network Embeddings (20%)\\n\"\n",
    "                \"- K-Means Clustering (10%)\"\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nEvaluation Results for {algo_details}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        k_values = [3, 5, 10]\n",
    "        for k in k_values:\n",
    "            print(f\"\\nFor k = {k}:\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Precision: {metrics[f'precision@{k}']:.3f}\")\n",
    "            print(f\"Recall   : {metrics[f'recall@{k}']:.3f}\")\n",
    "            print(f\"F1 Score : {metrics[f'f1@{k}']:.3f}\")\n",
    "\n",
    "    def create_hybrid_recommendations(self, customer_profiles, target_customers, n_recommendations=3):\n",
    "        \"\"\"\n",
    "        Create recommendations using multiple ML approaches and combine them\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # 1. Matrix Factorization using SVD\n",
    "        user_item_matrix = pd.pivot_table(\n",
    "            self.train_transactions,\n",
    "            values='TotalValue',\n",
    "            index='CustomerID',\n",
    "            columns='ProductID',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Normalize the matrix\n",
    "        matrix = user_item_matrix.values\n",
    "        user_ratings_mean = np.mean(matrix, axis=1)\n",
    "        matrix_normalized = matrix - user_ratings_mean.reshape(-1, 1)\n",
    "        \n",
    "        # SVD\n",
    "        U, sigma, Vt = svds(matrix_normalized, k=20)  # Reduced dimensions\n",
    "        sigma = np.diag(sigma)\n",
    "        user_features = np.dot(U, np.sqrt(sigma))\n",
    "        \n",
    "        # 2. Neural Network Embeddings\n",
    "        input_dim = customer_profiles.shape[1]\n",
    "        inputs = tf.keras.Input(shape=(input_dim,))\n",
    "        x = tf.keras.layers.Dense(64, activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        outputs = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "        \n",
    "        embedding_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        customer_embeddings = embedding_model.predict(customer_profiles.values)\n",
    "        \n",
    "        # 3. Clustering\n",
    "        kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "        clusters = kmeans.fit_predict(customer_profiles)\n",
    "        cluster_df = pd.DataFrame({'cluster': clusters}, index=customer_profiles.index)\n",
    "        \n",
    "        # Combine features for final similarity calculation\n",
    "        combined_features = np.hstack([\n",
    "            customer_profiles.values * 0.4,  # Traditional features\n",
    "            user_features * 0.3,            # Matrix factorization features\n",
    "            customer_embeddings * 0.2,      # Neural embeddings\n",
    "            np.eye(len(clusters))[clusters] * 0.1  # One-hot encoded clusters\n",
    "        ])\n",
    "        \n",
    "        # Calculate final similarity\n",
    "        similarity_matrix = cosine_similarity(combined_features)\n",
    "        similarity_df = pd.DataFrame(\n",
    "            similarity_matrix,\n",
    "            index=customer_profiles.index,\n",
    "            columns=customer_profiles.index\n",
    "        )\n",
    "        \n",
    "        # Get recommendations\n",
    "        for customer in target_customers:\n",
    "            if customer not in similarity_df.index:\n",
    "                continue\n",
    "            \n",
    "            # Get similarity scores excluding self\n",
    "            scores = similarity_df[customer].sort_values(ascending=False)\n",
    "            scores = scores[scores.index != customer]\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_similar = scores[:n_recommendations]\n",
    "            results[customer] = [(cust, round(score, 3)) for cust, score in top_similar.items()]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Recommendations...\n",
      "\n",
      "Evaluation Results for Traditional Method (Weighted Combination):\n",
      "- RFM Similarity (30%)\n",
      "- Price-based Similarity (30%)\n",
      "- Category Preference Similarity (30%)\n",
      "- Regional Similarity (10%)\n",
      "==================================================\n",
      "\n",
      "For k = 3:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.049\n",
      "F1 Score : 0.046\n",
      "\n",
      "For k = 5:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.049\n",
      "F1 Score : 0.046\n",
      "\n",
      "For k = 10:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.049\n",
      "F1 Score : 0.046\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738059468.633384   96407 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results for Hybrid ML Method:\n",
      "- Traditional Features (40%)\n",
      "- Matrix Factorization/SVD (30%)\n",
      "- Neural Network Embeddings (20%)\n",
      "- K-Means Clustering (10%)\n",
      "==================================================\n",
      "\n",
      "For k = 3:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.026\n",
      "F1 Score : 0.035\n",
      "\n",
      "For k = 5:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.026\n",
      "F1 Score : 0.035\n",
      "\n",
      "For k = 10:\n",
      "--------------------\n",
      "Precision: 0.051\n",
      "Recall   : 0.026\n",
      "F1 Score : 0.035\n",
      "\n",
      "Example Recommendations:\n",
      "==================================================\n",
      "\n",
      "Customer C0001:\n",
      "Cosine Similarity with Feature Weights:\n",
      "  - C0181 (similarity: 0.811)\n",
      "  - C0069 (similarity: 0.740)\n",
      "  - C0077 (similarity: 0.717)\n",
      "\n",
      "Ensemble of ML Models:\n",
      "  - C0051 (similarity: 0.919)\n",
      "  - C0184 (similarity: 0.818)\n",
      "  - C0194 (similarity: 0.739)\n",
      "\n",
      "Customer C0002:\n",
      "Cosine Similarity with Feature Weights:\n",
      "  - C0097 (similarity: 0.814)\n",
      "  - C0128 (similarity: 0.798)\n",
      "  - C0076 (similarity: 0.736)\n",
      "\n",
      "Ensemble of ML Models:\n",
      "  - C0138 (similarity: 0.848)\n",
      "  - C0117 (similarity: 0.777)\n",
      "  - C0079 (similarity: 0.776)\n",
      "\n",
      "Customer C0003:\n",
      "Cosine Similarity with Feature Weights:\n",
      "  - C0095 (similarity: 0.780)\n",
      "  - C0166 (similarity: 0.754)\n",
      "  - C0091 (similarity: 0.651)\n",
      "\n",
      "Ensemble of ML Models:\n",
      "  - C0096 (similarity: 0.631)\n",
      "  - C0072 (similarity: 0.550)\n",
      "  - C0192 (similarity: 0.531)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = LookalikeModel()\n",
    "    customer_profiles = model.create_customer_profiles()\n",
    "    target_customers = [f'C{str(i).zfill(4)}' for i in range(1, 21)]\n",
    "    \n",
    "    print(\"\\nGenerating Recommendations...\")\n",
    "    trad_recommendations = model.get_lookalikes(customer_profiles, target_customers)\n",
    "    trad_metrics = model.evaluate_recommendations(trad_recommendations, model.test_transactions)\n",
    "    model.print_evaluation_results(trad_metrics, \"Traditional\")\n",
    "    \n",
    "    hybrid_recommendations = model.create_hybrid_recommendations(customer_profiles, target_customers)\n",
    "    hybrid_metrics = model.evaluate_recommendations(hybrid_recommendations, model.test_transactions)\n",
    "    model.print_evaluation_results(hybrid_metrics, \"Hybrid\")\n",
    "    \n",
    "    # Print example recommendations\n",
    "    print(\"\\nExample Recommendations:\")\n",
    "    print(\"=\" * 50)\n",
    "    for customer in target_customers[:3]:\n",
    "        print(f\"\\nCustomer {customer}:\")\n",
    "        print(\"Cosine Similarity with Feature Weights:\")\n",
    "        if customer in trad_recommendations:\n",
    "            for rec_customer, score in trad_recommendations[customer]:\n",
    "                print(f\"  - {rec_customer} (similarity: {score:.3f})\")\n",
    "        print(\"\\nEnsemble of ML Models:\")\n",
    "        if customer in hybrid_recommendations:\n",
    "            for rec_customer, score in hybrid_recommendations[customer]:\n",
    "                print(f\"  - {rec_customer} (similarity: {score:.3f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
